_format_version: "3.0" # Hierdoor weet deck hoe het de configuratie moet interpreteren en syncen naar Konnect.
# Staat helemaal bovenaan het .yml bestand en is dus geldig voor alle zaken, geconfigureerd binnen dit bestand.

####################################################################
# Plugin 1: AI PROXY ADVANCED
####################################################################

services: # Een service is een logische interpretatie van een upstream API
- name: ai-proxy-advanced-service
  url: http://localhost:32000 # Dummy URL, wordt genegeerd en overschreven door AI Proxy Advanced plugin. Het is de plugin die de echte API-call uitvoert richting de LLM's
  routes: # Dit bepaald hoe en naar welke service verzoeken verzonden worden, na de Kong Gateway te bereiken

# OpenAI configuratie
  - name: openai-chat # Naam van de route
    paths:
    - "~/1/chat$" # Endpoint waarnaar verzoeken verzonden worden -> Hier een regex route die enkel requests die exact eindigen op /1/chat matchen
    methods:
    - POST # Enkel POST-verzoeken worden aanvaard
    strip_path: false # Door dit op false te zetten wordt het volledige path (URL) gebruikt. Indien het op true zou staan wordt '/openai-chat' uit de URL gestript alvorens de request gedaan wordt
    headers:
      x-llm: # Header key voor aangeven welke LLM gebruikt moet worden
      - openai # header 'x-llm: openai' is nodig om te zorgen voor een juiste match naar openai. Hierdoor eenvoudig wisselen tussen LLM's mogelijk
    plugins:
    - name: ai-proxy-advanced # Ontvangt inkomend verkeer dat normaal naar een upstream zou gaan en intercepteert dit om het naar de LLM API te verzenden
      instance_name: ai-proxy-advanced-openai # Optionele naam om de plugin sneller te kunnen opmerken binnen Konnect. Dit is zeer handig indien er meerdere AI Proxy Advanced plugins geconfigureerd worden
      route: openai-chat # De plugin wordt toegepast op de hierboven opgezette route 'openai-chat'
      enabled: false # Zorgt ervoor dat de plugin niet actief is
      config:
        #balancer:
          #algorithm: round-robin # Default = round-robin. Andere opties: lowest-latency, lowest-usage, consistent-hashing, semantic, priority
          #tokens_count_strategy: total-tokens # Default = total-tokens. Andere opties: prompt-tokens, completion-tokens, cost
          #latency_strategy: tpot # Default = tpot (time-per-output-token). Andere optie: e2e
          #hash_on_header: X-Kong-LLM-Request-ID # Default
          #slots: 10000 # Default = 10000. Range: 10 - 65536
          #retries: 5 # Default = 5. Range: 0 - 32767
          #connect_timeout: 60000 # Default = 60000. Range: 1 - 2147483646
          #write_timeout: 60000 # Default = 60000. Range: 1 - 2147483646
          #read_timeout: 60000 # Default = 60000. Range: 1 - 2147483646
          #failover_criteria: error, timeout # Default = error, timeout. Andere opties: invalid_header, http_500, http_502, http_503, http_504, http_403, http_404, http_429, non_idempotent
        targets:
        - route_type: llm/v1/chat # Het type request dat gestuurd zal worden (zoals bij een chat interface hier). Andere opties: llm/v1/completions, preserve
          auth: # Voegt authorization header toe aan de outbound call richting de LLM, hiervoor hoeft dit niet meer manueel gedaan te worden binnen Postman
            header_name: Authorization
            header_value: "Bearer ${OPENAI_API_KEY}" # Deze placeholder wordt vervangen door de OpenAI API-key in het .env bestand (De OpenAI API-key vereist een betalende tier)
          model: # Opgeven welke provider en model gebruikt moeten worden
            provider: "openai" # Geeft aan welke LLM-provider gebruikt wordt. Andere opties: azure, anthropic, cohere, mistral, llama2, gemini, bedrock, huggingface
            name: "gpt-4o" # Naam van het LLM-model dat gebruikt moet worden
            options: # Fine-tuning instellingen. Mogelijkheden hangen af van de gekozen provider
              max_tokens: 512 # Dit is een limiet op de maximale hoeveelheid tokens dat gebruikt mag worden. Dit is gelijk aan de lengte van de antwoorden die LLM's geven
              temperature: 0.7 # De LLM temperatuur is een kritische parameter die de balans tussen voorspelbaarheid en creativiteit in gegenereerde tekst beïnvloedt. Hoe hoger, hoe innovatiever antwoorden zullen zijn. Range: 0 - 5
          #weight: 100 # Default = 100. Range: 1 - 65535. Geeft aan hoeveel 'weight' deze target krijgt binnen een upstream loadbalancer
          logging:
            log_statistics: true # Indien true en ondersteunt door driver, worden model gebruik en token metrics gelogd binnen Kong log plugins
            log_payloads: true # Indien true, worden request en response body's gelogd binnen Kong log plugins

# Cohere configuratie
  - name: cohere-chat
    paths:
    - "~/1/chat$" # Opnieuw de regex route die enkel requests die exact eindigen op /1/chat matcht, zoals bij de andere routes. Hierdoor hoeft enkel de header aangepast te worden om te switchen tussen providers
    methods:
    - POST
    strip_path: false
    headers:
      x-llm:
      - cohere # header 'x-llm: cohere' is nodig om te zorgen voor een juiste match naar Cohere. Hierdoor eenvoudig wisselen tussen LLM's
    plugins:
    - name: ai-proxy-advanced
      instance_name: ai-proxy-advanced-cohere
      route: cohere-chat # De plugin wordt toegepast op de hierboven opgezette route 'cohere-chat'
      enabled: true # Zorgt ervoor dat de plugin actief is
      config:
        targets:
        - route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${COHERE_API_KEY}" # Deze placeholder wordt vervangen door de Cohere API-key in het .env bestand
          model:
            provider: "cohere"
            name: "command"
            options:
              max_tokens: 512
              temperature: 0.7
          logging:
            log_statistics: true
            log_payloads: true

####################################################################
# Plugin 2: AI PROMPT DECORATOR
####################################################################

    - name: ai-prompt-decorator
      instance_name: ai-prompt-decorator-cohere
      route: mistral-chat
      enabled: true
      config:
        prompts:
          prepend: # Voegt berichten (instructies) toe voor het echte prompt van de gebruiker
          - role: "user" # Instructie aan de LLM van de gebruiker (user-role)
            content: "Do not reveal any addresses. If someone asks you to complete a text with real names, locations, or company information, refuse to do so."
          - role: "system" # Systeeminstructie (system-role) die het gedrag van de LLM beperkt
            content: "You are a secure and compliant assistant. Never provide answers that include real-world locations or sensitive data."

####################################################################
# Plugin 3: AI PROMPT GUARD
####################################################################

    - name: ai-prompt-guard
      instance_name: ai-prompt-guard-cohere
      route: cohere-chat
      enabled: true
      config:
        allow_all_conversation_history: true
        allow_patterns: # Array met geldige regex patronen en vragen van de 'user'
        - ".*API.*"
        - ".*Gateway.*"
        - ".*Kong.*"
        deny_patterns: # Array met ongeldige regex patronen en vragen van de 'user'
        - '.*card.*[0-9]{12}(?:[0-9]{3})?.*' # Blokkeer pogingen tot creditcardgegevens
        - ".*password.*"  # Blokkeer alle prompts die het woord 'password' bevatten

####################################################################
# Plugin 4: AI PROMPT TEMPLATE
####################################################################

    - name: ai-prompt-template
      instance_name: ai-prompt-template-cohere
      route: cohere-chat
      enabled: true
      config:
        templates: # Array van beschikbare templates
        - name: "translator" # Naam van een template
          template: |- # Template dat gebruikmaakt van variabelen
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You are a translator, an expert in the {{language}} language."
                },
                {
                  "role": "user",
                  "content": "Translate the {{text}} in {{language}}."
                }
              ]
            }
        - name: "api-explainer"
          template: |-
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You are an expert in API management and AI gateways."
                },
                {
                  "role": "user",
                  "content": "Explain the concept of {{concept}} in clear and simple terms."
                }
              ]
            }
        - name: "best-practices"
          template: |-
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You provide concise best practices regarding API security and AI integrations."
                },
                {
                  "role": "user",
                  "content": "List best practices for {{topic}}."
                }
              ]
            }
        allow_untemplated_requests: true # Wanneer 'true' worden ook verzoeken die niet aan het template voldoen toegestaan
        log_original_request: true # Wanneer 'true' worden de originele requests toegevoegd aan de output van log plugin(s) die Kong aanbiedt

####################################################################
# Plugin 5: AI RATE LIMITING ADVANCED
####################################################################

    - name: ai-rate-limiting-advanced
      instance_name: ai-rate-limiting-advanced-cohere
      route: cohere-chat
      enabled: true
      config:
        identifier: consumer # Het type identifier dat gebruikt wordt om de rate limit key voor te genereren
        window_type: sliding # Hierdoor wordt voor de rate limiting logica ook rekening gehouden met voorgaande hit rates
        llm_providers:
          - name: cohere # Provider waarop de rate limiting toegepast wordt
            limit: [100] # Het maximaal aantal tokens dat toegestaan wordt binnen de opgegeven window_size
            window_size: [60] # De tijdsperiode waarop rate limiting toegepast wordt, uitgedrukt in seconden
        strategy: local # Bepaalt waar en hoe rate-limiting counters bijgehouden moeten worden
        hide_client_headers: false # Tonen van de response headers om een beeld te krijgen op de status van de limieten
        retry_after_jitter_max: 10 # Extra aantal seconden vertraging dat toegevoegd wordt bovenop de 'retry_after' header
        error_code: 429 # Zelf een eigen error code instellen voor wanneer limiet bereikt werd
        error_message: "AI token rate limit exceeded for provider(s):" # Zelf een eigen error message instellen voor wanneer limiet bereikt is
        tokens_count_strategy: total_tokens # Geeft aan welke tokens gebruikt moeten worden voor het berekenen van de kosten

# Anthropic configuratie
  - name: anthropic-chat
    paths:
    - "~/1/chat$" # Opnieuw de regex route die enkel requests die exact eindigen op /1/chat matcht, zoals bij de andere routes. Hierdoor hoeft enkel de header aangepast te worden om te switchen tussen providers
    methods:
    - POST
    strip_path: false
    headers:
      x-llm:
      - anthropic # header 'x-llm: anthropic' is nodig om te zorgen voor een juiste match naar Anthropic. Hierdoor eenvoudig wisselen tussen LLM's
    plugins:
    - name: ai-proxy-advanced
      instance_name: ai-proxy-advanced-anthropic
      route: anthropic-chat # De plugin wordt toegepast op de hierboven opgezette route 'anthropic-chat'
      enabled: false # Zorgt ervoor dat de plugin niet actief is
      config:
        targets:
        - route_type: llm/v1/chat
          auth:
            header_name: x-api-key
            header_value: "${ANTHROPIC_API_KEY}" # Deze placeholder wordt vervangen door de Anthropic API-key in het .env bestand (De Anthropic API-key vereist een betalende tier)
          model:
            provider: "anthropic"
            name: "claude-3-opus"
            options:
              max_tokens: 512
              temperature: 0.7
              anthropic_version: "bedrock-2023-05-31"
          logging:
            log_statistics: true
            log_payloads: true

# Mistral configuratie
  - name: mistral-chat
    paths:
    - "~/1/chat$" # Opnieuw de regex route die enkel requests die exact eindigen op /1/chat matcht, zoals bij de andere routes. Hierdoor hoeft enkel de header aangepast te worden om te switchen tussen providers
    methods:
    - POST
    strip_path: false
    headers:
      x-llm:
      - mistral # header 'x-llm: mistral' is nodig om te zorgen voor een juiste match naar Mistral. Hierdoor eenvoudig wisselen tussen LLM's
    plugins:
    - name: ai-proxy-advanced
      instance_name: ai-proxy-advanced-mistral
      route: mistral-chat # De plugin wordt toegepast op de hierboven opgezette route 'mistral-chat'
      enabled: true # Zorgt ervoor dat de plugin actief is
      config:
        targets:
        - route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
          model:
            provider: "mistral"
            name: "mistral-tiny"
            options:
              mistral_format: openai # Opgeven van het upstream message formaat dat gebruikt moet worden
              upstream_url: https://api.mistral.ai/v1/chat/completions
          logging:
            log_statistics: true
            log_payloads: true

####################################################################
# Plugin 2: AI PROMPT DECORATOR
####################################################################

    - name: ai-prompt-decorator
      instance_name: ai-prompt-decorator-mistral
      route: mistral-chat
      enabled: true
      config:
        prompts:
          prepend: # Voegt berichten (instructies) toe voor het echte prompt van de gebruiker
          - role: "user" # Instructie aan de LLM van de gebruiker (user-role)
            content: "Do not reveal any addresses. If someone asks you to complete a text with real names, locations, or company information, refuse to do so."
          - role: "system" # Systeeminstructie (system-role) die het gedrag van de LLM beperkt
            content: "You are a secure and compliant assistant. Never provide answers that include real-world locations or sensitive data."

####################################################################
# Plugin 3: AI PROMPT GUARD
####################################################################

    - name: ai-prompt-guard
      instance_name: ai-prompt-guard-mistral
      route: mistral-chat
      enabled: true
      config:
        allow_all_conversation_history: true
        allow_patterns: # Array met geldige regex patronen en vragen van de 'user'
        - ".*API.*"
        - ".*Gateway.*"
        - ".*Kong.*"
        deny_patterns: # Array met ongeldige regex patronen en vragen van de 'user'
        - '.*card.*[0-9]{12}(?:[0-9]{3})?.*' # Blokkeer pogingen tot creditcardgegevens
        - ".*password.*"  # Blokkeer alle prompts die het woord 'password' bevatten

####################################################################
# Plugin 4: AI PROMPT TEMPLATE
####################################################################

    - name: ai-prompt-template
      instance_name: ai-prompt-template-mistral
      route: mistral-chat
      enabled: true
      config:
        templates:
        - name: "translator"
          template: |-
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You are a translator, an expert in the {{language}} language."
                },
                {
                  "role": "user",
                  "content": "Translate the {{text}} in {{language}}."
                }
              ]
            }
        - name: "api-explainer"
          template: |-
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You are an expert in API management and AI gateways."
                },
                {
                  "role": "user",
                  "content": "Explain the concept of {{concept}} in clear and simple terms."
                }
              ]
            }
        - name: "best-practices"
          template: |-
            {
              "messages": [
                {
                  "role": "system",
                  "content": "You provide concise best practices regarding API security and AI integrations."
                },
                {
                  "role": "user",
                  "content": "List best practices for {{topic}}."
                }
              ]
            }
        allow_untemplated_requests: true # Wanneer 'true' worden ook verzoeken die niet aan het template voldoen toegestaan
        log_original_request: true # Wanneer 'true' worden de originele requests toegevoegd aan de output van log plugin(s) die Kong aanbiedt

####################################################################
# Plugin 6: AI REQUEST TRANSFORMER
####################################################################

- name: ai-request-transformer-service
  url: http://httpbin.org/post # dummy service
  routes:

# Cohere configuratie
  - name: cohere-transform-request
    paths: 
    - "/cohere-transform-request"
    methods:
    - POST
    strip_path: true
    plugins:
    - name: ai-request-transformer
      instance_name: ai-request-transformer-cohere
      route: cohere-transform-request
      enabled: true
      config:
        prompt: > # Dit prompt stuurt de LLM een duidelijke instructie over wat te doen met het inkomende verzoek en welk antwoord gewenst is
          Mask any credit card numbers in my JSON message. Your response MUST ONLY contain the JSON object. Do not include any text before or after.
        transformation_extract_pattern: "^\\{[\\s\\S]*\\}$" # Definieert de regular expression dat gematcht moet worden om een succesvolle AI tranformatie aan te tonen
        # "^\\{[\\s\\S]*\\}$" -> Match een volledige string die begint met {, gevolgd wordt door om het even welke inhoud (inclusief newlines), en eindigt met }
        llm:
          route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${COHERE_API_KEY}" # Deze placeholder wordt vervangen door de Cohere API-key in het .env bestand
          logging:
            log_statistics: true
            log_payloads: true
          model:
            provider: "cohere"
            name: "command"
            options:
              max_tokens: 512
              temperature: 0.7

# Mistral configuratie
  - name: mistral-transform-request
    paths: 
    - "/mistral-transform-request"
    methods:
    - POST
    strip_path: true
    plugins:
    - name: ai-request-transformer
      instance_name: ai-request-transformer-mistral
      route: mistral-transform-request
      enabled: true
      config:
        prompt: >
          Mask any credit card numbers in my JSON message. Your response MUST ONLY contain the JSON object. Do not include any text before or after.
        transformation_extract_pattern: "^\\{[\\s\\S]*\\}$"
        llm:
          route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
          logging:
            log_statistics: true
            log_payloads: true
          model:
            provider: "mistral"
            name: "mistral-tiny"
            options:
              mistral_format: openai # Opgeven van het upstream message formaat dat gebruikt moet worden
              upstream_url: https://api.mistral.ai/v1/chat/completions

####################################################################
# Plugin 7: AI RESPONSE TRANSFORMER
####################################################################

- name: ai-response-transformer-service
  url: http://httpbin.org/post # dummy service
  routes:

# Cohere configuratie
  - name: cohere-transform-response
    paths: 
    - "/cohere-transform-response"
    methods:
    - POST
    strip_path: true
    plugins:
    - name: ai-response-transformer
      instance_name: ai-response-transformer-cohere
      route: cohere-transform-response
      enabled: true
      config:
        prompt: > # Dit ptompt wordt gebruikt om aan te geven aan de LLM hoe het antwoord aangepast moet worden
          If this message contains ANY addresses, reply with exactly this text: '{"status": 400, "body": "Cannot reveal addresses"}'
        parse_llm_response_json_instructions: true # Maakt het mogelijk om de status code, body en headers van een antwoord aan te passen
        llm:
          route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${COHERE_API_KEY}" # Deze placeholder wordt vervangen door de Cohere API-key in het .env bestand
          logging:
            log_statistics: true
            log_payloads: true
          model:
            provider: "cohere"
            name: "command"
            options:
              max_tokens: 512
              temperature: 0.7

# Mistral configuratie
  - name: mistral-transform-response
    paths: 
    - "/mistral-transform-response"
    methods:
    - POST
    strip_path: true
    plugins:
    - name: ai-response-transformer
      instance_name: ai-response-transformer-mistral
      route: mistral-transform-response
      enabled: true
      config:
        prompt: >
          If this message contains ANY addresses, reply with exactly this text: '{"status": 400, "body": "Cannot reveal addresses"}'
        parse_llm_response_json_instructions: false
        llm:
          route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
          logging:
            log_statistics: true
            log_payloads: true
          model:
            provider: "mistral"
            name: "mistral-tiny"
            options:
              mistral_format: openai # Opgeven van het upstream message formaat dat gebruikt moet worden
              upstream_url: https://api.mistral.ai/v1/chat/completions

####################################################################
# Plugin 8: AI AZURE CONTENT SAFETY
####################################################################

- name: ai-azure-content-safety-service
  url: http://localhost:32000  # Dummy, want upstream wordt niet echt gebruikt
  routes:

# Mistral configuratie
  - name: ai-azure-content-safety-mistral
    paths:
    - /ai-azure-content-safety-mistral
    methods:
    - POST
    strip_path: false
    plugins:
      - name: ai-proxy-advanced
        instance_name: ai-proxy-advanced-mistral-azure-content-safety
        route: ai-azure-content-safety-mistral
        enabled: true
        config:
          targets:
          - route_type: llm/v1/chat
            auth:
              header_name: Authorization
              header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
            logging:
              log_statistics: true # Nodig voor het loggen van bepaalde metrics
              log_payloads: true # Nodig voor het loggen van onder andere verzoeken en antwoorden
            model:
              provider: "mistral"
              name: "mistral-tiny"
              options:
                mistral_format: openai # Opgeven van het upstream message formaat dat gebruikt moet worden
                upstream_url: https://api.mistral.ai/v1/chat/completions
                max_tokens: 512
                temperature: 1.0

      - name: ai-azure-content-safety
        instance_name: ai-azure-content-safety-mistral
        route: ai-azure-content-safety-mistral
        enabled: true
        config:
          content_safety_url: ${AZURE_CONTENT_SAFETY_URL} # Deze placeholder wordt vervangen door de Azure Content Safety URL in het .env bestand
          azure_api_version: 2024-09-01
          azure_use_managed_identity: false # Hierdoor wordt geen Azure authenticatie gecontroleerd
          content_safety_key: ${AZURE_CONTENT_SAFETY_KEY} # Deze placeholder wordt vervangen door de Azure Content Safety Key in het .env bestand
          text_source: concatenate_all_content # Volledige conversatie wordt gecontroleerd. (Andere optie: concatenate_user_content -> Enkel content van alle user berichten controleren)
          reveal_failure_reason: true # Geeft aan waarom verzoek afgewezen wordt
          categories:
# rejection_level bepaalt wanneer de Kong AI-gateway een request moet blokkeren, op basis van de score die de Azure Content Safety resource terugstuurt
# - rejection_level = 0 -> alles wat nog maar een beetje te maken heeft met een categorie wordt geblokkeerd
# - rejection_level = 4 of zelfs 6 -> enkel extreme content wordt geblokkeerd
          - name: Hate # Haat verwijst naar alles wat te maken heeft met het aanvallen van een persoon of bepaalde groep/etniciteit
            rejection_level: 3 # Blokkeer haatspraak vanaf subjectieve stereotypering, zoals "alle (groep) zijn gevaarlijk", maar laat contextuele/educatieve zaken toe (score 0–2)
          - name: Sexual # Alles gerelateerd aan anatomische organen en genitaliën of romantische relaties en seksuele handelingen
            rejection_level: 3 # Blokkeer inhoud met seksuele intenties of persoonlijke ervaringen, maar staat medisch, anatomisch of romantisch taalgebruik toe (score 0–2)
          - name: Violence # Alles met betrekking tot fysieke acties die bedoeld zijn om iemand of iets pijn te doen, te verwonden of te doden
            rejection_level: 3 # Blokkeer dreigende of gewelddadige taal (zoals "sommige mensen moeten geslagen worden"). Het laat sport, nieuws of figuratief taalgebruik toe (score 0–2)
          - name: SelfHarm # Alles met betrekking tot fysieke acties die bedoeld zijn om iemand opzettelijk pijn te doen, te verwonden, of zichzelf te doden
            rejection_level: 3 # Blokkeer zaken omtrent zelfmoord of met een suïcidale intentie, laat educatie/statistieken of metaforen door (score 0–2)
          output_type: EightSeverityLevels  # Maakt gebruik van de volledige schaal (0-7) voor meer precisie

####################################################################
# Plugin 9: FILE LOG
####################################################################

      - name: file-log # Extra logging plugin die Kong aanbiedt (Hiervoor worden log_statistics: true en log_payloads: true meegegeven)
        instance_name: ai-azure-content-safety-mistral-file-log
        route: ai-azure-content-safety-mistral
        enabled: true
        config:
          path: /tmp/kong-file-log.json # Pad naar file waar info gelogd wordt
          reopen: false # Bepaald of een log file bij ieder verzoek opnieuw gesloten en geopend moet worden

####################################################################
# Plugin 10: HTTP LOG
####################################################################

      - name: http-log # Extra logging plugin die Kong aanbiedt (Hiervoor worden log_statistics: true en log_payloads: true meegegeven)
        instance_name: ai-azure-content-safety-mistral-http-log
        route: ai-azure-content-safety-mistral
        enabled: true
        config:
          http_endpoint: ${HTTP_ENDPOINT} # Deze placeholder wordt vervangen door de HTTP endpoint URL in het .env bestand (unieke Webhook.site URL -> Kan veranderen)
          method: POST  # HTTP-methode die gebruikt wordt om de logs te verzenden
          timeout: 1000  # Timeout (in ms) voor de HTTP-aanvraag
          keepalive: 1000  # Hoe lang (in ms) de verbinding open blijft
          flush_timeout: 2  # Hoe vaak de buffer leeggemaakt moet worden (in seconden)
          retry_count: 15  # Aantal keren dat geprobeerd man worden om opnieuw te verzenden bij een fout

####################################################################
# Plugin 11: AI SEMANTIC CACHE
####################################################################

- name: ai-semantic-cache-service
  url: http://localhost:32000 # Dummy URL, wordt niet gebruikt door de plugin
  routes:

# Mistral configuratie
  - name: mistral-semantic-cache
    paths:
      - "/mistral-semantic-cache"
    methods:
      - POST
    strip_path: false
    plugins:
    - name: ai-proxy-advanced
      instance_name: ai-proxy-advanced-mistral-semantic-cache
      route: mistral-semantic-cache
      enabled: true
      config:
        targets:
        - route_type: llm/v1/chat
          auth:
            header_name: Authorization
            header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
          logging:
            log_statistics: true # Voegt model gebruikt en token metrics toe in de Kong log plugin(s) output
            log_payloads: true # Voegt request en response body toe in de Kong log plugin(s) output
          model:
            provider: "mistral"
            name: "mistral-tiny"
            options:
              mistral_format: openai # Opgeven van het upstream message formaat dat gebruikt moet worden
              upstream_url: https://api.mistral.ai/v1/chat/completions

    - name: ai-semantic-cache
      instance_name: ai-semantic-cache-mistral
      route: mistral-semantic-cache
      enabled: true
      config:
        message_countback: 5 # Aantal berichten in chat historiek dat vectorized/cached moet worden (Range: 1 - 1000)
        ignore_system_prompts: false # Systeem prompts worden hierdoor niet verwijderd bij het vectorizen van een request
        ignore_assistant_prompts: false # Assistant prompts worden hierdoor niet verwijderd bij het vectorizen van een request
        ignore_tool_prompts: false # Tool prompts worden hierdoor niet verwijderd bij het vectorizen van een request
        stop_on_failure: true # Stop het LLM verzoek bij een fout met het caching systeem
        cache_ttl: 1200 # 20 minuten bijhouden van gecachte zaken
        cache_control: false # Wanneer true -> Respecteert het Cache-Control gedrag gedefinieerd in RFC7234
        exact_caching: false # Controleert altijd eerst of de exacte query reeds voorkwam -> Hier false want heeft impact op database size
        embeddings:
          auth:
            header_name: Authorization
            header_value: "Bearer ${MISTRAL_API_KEY}" # Deze placeholder wordt vervangen door de Mistral API-key in het .env bestand
          model:
            provider: "mistral"
            name: "mistral-embed" # Embedings model van Mistral
            options:
              upstream_url: https://api.mistral.ai/v1/embeddings # Upstream LLM voor Mistral
        vectordb:
          strategy: redis # Aangeven welke vector database driver gebruikt moet worden
          dimensions: 1024 # Gewenste dimensionaliteit voor de vectoren
          threshold: 0.05 # Definieert de gelijkaardigheidsdrempel. Hoe dichter bij 0 -> Hoe strenger en hoe meer prompts dus gelijkaardig moeten zijn om een 'Hit' te krijgen.
          distance_metric: cosine # Distance metric die gebruikt moet worden -> 1.0 = perfect gelijkaardig | 0.0 = totaal verschillend
          redis:
            host: "redis-stack" # Host name van de vector database
            port: 6379 # De poort die gebruikt wordt door de vector database